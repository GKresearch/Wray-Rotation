{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b086488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 13 14:21:08 2022\n",
    "\n",
    "@author: jamesonblount\n",
    "\"\"\"\n",
    "\n",
    "# In[]:\n",
    "# Importing required packages\n",
    "#Importing basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing sklearn modules\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
    "from sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717ec477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqnames                  0\n",
       "start                     0\n",
       "end                       0\n",
       "width                     0\n",
       "strand                    0\n",
       "                         ..\n",
       "gene.y                    0\n",
       "dTSS                      0\n",
       "PhastCons                 0\n",
       "PhyloP_primates_score     0\n",
       "PhyloP_placental_score    0\n",
       "Length: 90, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data and checking for missing values\n",
    "dataset=pd.read_csv('C:/Users/ictinike/Documents/WrayLab/raw_data/x_0011_df_phyloP.csv')\n",
    "dataset.isnull().sum()\n",
    "\n",
    "datasetv2 = dataset.dropna(axis=1)\n",
    "datasetv2.isnull().sum()\n",
    "# Checking the data set for any NULL values is very essential, as MLAs can not \n",
    "# handle NULL values. We have to either eliminate the records with NULL values \n",
    "# or replace them with the mean/median of the other values. we can see each of \n",
    "# the variables are printed with number of null values. This data set has no null \n",
    "# values so all are zero here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b57d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Active Promoter', 'Candidate Strong Enhancer',\n",
       "        'Candidate Weak Enhancer', 'Distal CTCF/Candidate Insulator',\n",
       "        'Heterochromatin/Repetitive/Copy Number Variation',\n",
       "        'Inactive Promoter', 'Low activity proximal to active states',\n",
       "        'Polycomb repressed', 'Promoter Flanking',\n",
       "        'Transcription asociated'], dtype=object)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the categorical variables into numerical\n",
    "# Instantiate OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit_transform(datasetv2[[\"chromHMM_cat_longest\"]])[:5]\n",
    "datasetv2['chromHMM_cat_longest'].head()\n",
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7275d6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(datasetv2.chromHMM_cat_longest.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84160b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      seqnames      start        end  width strand       name  score  \\\n",
      "0         chr1     793301     793692    392      *     chr1.7    653   \n",
      "1         chr1     793779     794382    604      *     chr1.8    553   \n",
      "2         chr1     846424     847133    710      *    chr1.17    867   \n",
      "3         chr1     847379     847941    563      *    chr1.18    544   \n",
      "4         chr1     848207     849945   1739      *    chr1.19    605   \n",
      "...        ...        ...        ...    ...    ...        ...    ...   \n",
      "80348     chrX  155110643  155111395    753      *  chrX.2827   1000   \n",
      "80349     chrX  155196562  155196941    380      *  chrX.2829    677   \n",
      "80350     chrX  155227040  155227522    483      *  chrX.2831    619   \n",
      "80351     chrX  155231134  155231652    519      *  chrX.2832    745   \n",
      "80352     chrX  155232085  155232595    511      *  chrX.2833    688   \n",
      "\n",
      "       signalValue  pValue  qValue  ...  annotation_5' UTR  \\\n",
      "0           0.0677    3.87      -1  ...                  0   \n",
      "1           0.0359    1.98      -1  ...                  0   \n",
      "2           0.1357    7.97      -1  ...                  0   \n",
      "3           0.0332    1.82      -1  ...                  0   \n",
      "4           0.0525    2.96      -1  ...                  0   \n",
      "...            ...     ...     ...  ...                ...   \n",
      "80348       0.1998   11.90      -1  ...                  0   \n",
      "80349       0.0752    4.32      -1  ...                  0   \n",
      "80350       0.0570    3.23      -1  ...                  0   \n",
      "80351       0.0969    5.63      -1  ...                  1   \n",
      "80352       0.0789    4.54      -1  ...                  1   \n",
      "\n",
      "       annotation_Distal Intergenic  annotation_Downstream (1-2kb)  \\\n",
      "0                                 0                              0   \n",
      "1                                 0                              0   \n",
      "2                                 0                              0   \n",
      "3                                 0                              0   \n",
      "4                                 0                              0   \n",
      "...                             ...                            ...   \n",
      "80348                             0                              0   \n",
      "80349                             1                              0   \n",
      "80350                             0                              0   \n",
      "80351                             0                              0   \n",
      "80352                             0                              0   \n",
      "\n",
      "       annotation_Downstream (2-3kb)  annotation_Downstream (<1kb)  \\\n",
      "0                                  0                             0   \n",
      "1                                  0                             0   \n",
      "2                                  0                             0   \n",
      "3                                  0                             0   \n",
      "4                                  0                             0   \n",
      "...                              ...                           ...   \n",
      "80348                              0                             0   \n",
      "80349                              0                             0   \n",
      "80350                              0                             0   \n",
      "80351                              0                             0   \n",
      "80352                              0                             0   \n",
      "\n",
      "       annotation_Exon  annotation_Intron  annotation_Promoter (1-2kb)  \\\n",
      "0                    1                  0                            0   \n",
      "1                    1                  0                            0   \n",
      "2                    1                  0                            0   \n",
      "3                    1                  0                            0   \n",
      "4                    1                  0                            0   \n",
      "...                ...                ...                          ...   \n",
      "80348                0                  0                            0   \n",
      "80349                0                  0                            0   \n",
      "80350                0                  0                            0   \n",
      "80351                0                  0                            0   \n",
      "80352                0                  0                            0   \n",
      "\n",
      "       annotation_Promoter (2-3kb)  annotation_Promoter (<=1kb)  \n",
      "0                                0                            0  \n",
      "1                                0                            0  \n",
      "2                                0                            0  \n",
      "3                                0                            0  \n",
      "4                                0                            0  \n",
      "...                            ...                          ...  \n",
      "80348                            0                            1  \n",
      "80349                            0                            0  \n",
      "80350                            0                            1  \n",
      "80351                            0                            0  \n",
      "80352                            0                            0  \n",
      "\n",
      "[80353 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoded_data = pd.get_dummies(datasetv2, columns = ['chromHMM_cat_longest','annotation'])\n",
    "print(one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc01a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ictinike\\AppData\\Local\\Temp\\ipykernel_1284\\3935728989.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  datasetv2['score_quart'] = pd.qcut(datasetv2['score'], q = 4, labels = bin_labels)\n"
     ]
    }
   ],
   "source": [
    "# Compare performance of ML between top quartile to bottom quartile of AUC (best)/peak size\n",
    "bin_labels = ['Lower', 'Midlower','Midupper', 'Upper']\n",
    "lower_bin = ['Lower']\n",
    "upper_bin = ['Upper']\n",
    "datasetv2['score_quart'] = pd.qcut(datasetv2['score'], q = 4, labels = bin_labels)\n",
    "datasetLower = datasetv2[datasetv2['score_quart'].isin(lower_bin)]\n",
    "datasetUpper = datasetv2[datasetv2['score_quart'].isin(upper_bin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78e3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetv2[\"dhs_0_1_wg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05dd7fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(sparse=False),\n",
       "                                 ['chromHMM_cat_longest', 'annotation'])])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df81a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b77b6e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 LM predictions:  [0.34375, 0.28125, 0.24609375, 0.34375, 0.07421875]\n"
     ]
    }
   ],
   "source": [
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "755e8302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34375   ,  0.28125   ,  0.24609375, ...,  0.31640625,\n",
       "       -0.8671875 ,  0.34375   ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53886202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=pd.get_dummies(x, columns = ['chromHMM_cat_longest','annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faecdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29e83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of all Machine Learning methods\n",
    "MLA = [\n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(max_iter=2000),\n",
    "    linear_model.PassiveAggressiveClassifier(max_iter=2000),\n",
    "    linear_model.SGDClassifier(max_iter=2000),\n",
    "    linear_model.Perceptron(max_iter=2000),\n",
    "    \n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    #svm.LinearSVC(max_iter=2000),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "  \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #Others\n",
    "    neural_network.MLPClassifier(hidden_layer_sizes = (512, 256, 128, 64), max_iter=10000),\n",
    "    #neural_network.MLPRegressor(max_iter=2000),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c802fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a dataframe to visualize comparison betweeen all algorithms\n",
    "MLA_columns = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a8c900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHS_prop_repeat</th>\n",
       "      <th>DHS_prop_GC</th>\n",
       "      <th>DHS_length</th>\n",
       "      <th>n_SNV_Zhou_per_bp</th>\n",
       "      <th>distanceToTSS</th>\n",
       "      <th>zeta.human</th>\n",
       "      <th>zeta.chimp</th>\n",
       "      <th>PP_con</th>\n",
       "      <th>PP_acc</th>\n",
       "      <th>PhastCons</th>\n",
       "      <th>chromHMM_cat_longest</th>\n",
       "      <th>annotation</th>\n",
       "      <th>PhyloP_primates_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9442</th>\n",
       "      <td>0.229236</td>\n",
       "      <td>0.588040</td>\n",
       "      <td>301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12820</td>\n",
       "      <td>9.373772e-01</td>\n",
       "      <td>1.033511e+00</td>\n",
       "      <td>0.918973</td>\n",
       "      <td>0.680151</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>Candidate Strong Enhancer</td>\n",
       "      <td>Intron</td>\n",
       "      <td>-0.027056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17819</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650224</td>\n",
       "      <td>223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1162</td>\n",
       "      <td>3.161790e-14</td>\n",
       "      <td>3.428107e-14</td>\n",
       "      <td>0.846055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>Candidate Weak Enhancer</td>\n",
       "      <td>Promoter (1-2kb)</td>\n",
       "      <td>0.154713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79682</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585565</td>\n",
       "      <td>859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-14597</td>\n",
       "      <td>3.515203e-01</td>\n",
       "      <td>2.898497e-01</td>\n",
       "      <td>0.625180</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>Candidate Weak Enhancer</td>\n",
       "      <td>Intron</td>\n",
       "      <td>0.061763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79332</th>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.535461</td>\n",
       "      <td>564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-26842</td>\n",
       "      <td>3.047240e-01</td>\n",
       "      <td>8.448791e-01</td>\n",
       "      <td>0.122381</td>\n",
       "      <td>0.994416</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>Distal CTCF/Candidate Insulator</td>\n",
       "      <td>Intron</td>\n",
       "      <td>-0.172231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65221</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>485</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>352470</td>\n",
       "      <td>1.169852e+00</td>\n",
       "      <td>8.143365e-01</td>\n",
       "      <td>0.659730</td>\n",
       "      <td>0.759748</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Polycomb repressed</td>\n",
       "      <td>3' UTR</td>\n",
       "      <td>-0.031473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67558</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10114</td>\n",
       "      <td>3.499595e+00</td>\n",
       "      <td>3.847540e-01</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Candidate Strong Enhancer</td>\n",
       "      <td>Intron</td>\n",
       "      <td>-0.027879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17200</th>\n",
       "      <td>0.133739</td>\n",
       "      <td>0.462006</td>\n",
       "      <td>329</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>-78291</td>\n",
       "      <td>1.856400e-14</td>\n",
       "      <td>2.788474e-01</td>\n",
       "      <td>0.792861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Candidate Weak Enhancer</td>\n",
       "      <td>Intron</td>\n",
       "      <td>0.327204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61376</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.446108</td>\n",
       "      <td>334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-160135</td>\n",
       "      <td>1.049991e+00</td>\n",
       "      <td>7.475919e-01</td>\n",
       "      <td>0.906381</td>\n",
       "      <td>0.467250</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>Candidate Strong Enhancer</td>\n",
       "      <td>Distal Intergenic</td>\n",
       "      <td>-0.041890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47714</th>\n",
       "      <td>0.478114</td>\n",
       "      <td>0.441077</td>\n",
       "      <td>297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-496</td>\n",
       "      <td>4.586345e-01</td>\n",
       "      <td>9.092983e-01</td>\n",
       "      <td>0.644118</td>\n",
       "      <td>0.897621</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>Active Promoter</td>\n",
       "      <td>Promoter (&lt;=1kb)</td>\n",
       "      <td>-0.080085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63770</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533981</td>\n",
       "      <td>206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62567</td>\n",
       "      <td>2.544209e-14</td>\n",
       "      <td>2.523063e-14</td>\n",
       "      <td>0.860062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>Distal CTCF/Candidate Insulator</td>\n",
       "      <td>Intron</td>\n",
       "      <td>0.201253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16071 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DHS_prop_repeat  DHS_prop_GC  DHS_length  n_SNV_Zhou_per_bp  \\\n",
       "9442          0.229236     0.588040         301           0.000000   \n",
       "17819         0.000000     0.650224         223           0.000000   \n",
       "79682         0.000000     0.585565         859           0.000000   \n",
       "79332         0.042553     0.535461         564           0.000000   \n",
       "65221         0.000000     0.620619         485           0.006186   \n",
       "...                ...          ...         ...                ...   \n",
       "67558         0.000000     0.521739         391           0.000000   \n",
       "17200         0.133739     0.462006         329           0.003040   \n",
       "61376         1.000000     0.446108         334           0.000000   \n",
       "47714         0.478114     0.441077         297           0.000000   \n",
       "63770         0.000000     0.533981         206           0.000000   \n",
       "\n",
       "       distanceToTSS    zeta.human    zeta.chimp    PP_con    PP_acc  \\\n",
       "9442           12820  9.373772e-01  1.033511e+00  0.918973  0.680151   \n",
       "17819           1162  3.161790e-14  3.428107e-14  0.846055  1.000000   \n",
       "79682         -14597  3.515203e-01  2.898497e-01  0.625180  0.833042   \n",
       "79332         -26842  3.047240e-01  8.448791e-01  0.122381  0.994416   \n",
       "65221         352470  1.169852e+00  8.143365e-01  0.659730  0.759748   \n",
       "...              ...           ...           ...       ...       ...   \n",
       "67558          10114  3.499595e+00  3.847540e-01  0.999994  0.000255   \n",
       "17200         -78291  1.856400e-14  2.788474e-01  0.792861  1.000000   \n",
       "61376        -160135  1.049991e+00  7.475919e-01  0.906381  0.467250   \n",
       "47714           -496  4.586345e-01  9.092983e-01  0.644118  0.897621   \n",
       "63770          62567  2.544209e-14  2.523063e-14  0.860062  1.000000   \n",
       "\n",
       "       PhastCons             chromHMM_cat_longest         annotation  \\\n",
       "9442      0.0010        Candidate Strong Enhancer             Intron   \n",
       "17819     0.0085          Candidate Weak Enhancer   Promoter (1-2kb)   \n",
       "79682     0.0110          Candidate Weak Enhancer             Intron   \n",
       "79332     0.0020  Distal CTCF/Candidate Insulator             Intron   \n",
       "65221     0.0000               Polycomb repressed             3' UTR   \n",
       "...          ...                              ...                ...   \n",
       "67558     0.0000        Candidate Strong Enhancer             Intron   \n",
       "17200     1.0000          Candidate Weak Enhancer             Intron   \n",
       "61376     0.0010        Candidate Strong Enhancer  Distal Intergenic   \n",
       "47714     0.0010                  Active Promoter   Promoter (<=1kb)   \n",
       "63770     0.6310  Distal CTCF/Candidate Insulator             Intron   \n",
       "\n",
       "       PhyloP_primates_score  \n",
       "9442               -0.027056  \n",
       "17819               0.154713  \n",
       "79682               0.061763  \n",
       "79332              -0.172231  \n",
       "65221              -0.031473  \n",
       "...                      ...  \n",
       "67558              -0.027879  \n",
       "17200               0.327204  \n",
       "61376              -0.041890  \n",
       "47714              -0.080085  \n",
       "63770               0.201253  \n",
       "\n",
       "[16071 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfb9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm  = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a0f5ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Candidate Strong Enhancer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1284\\2037246093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    663\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Candidate Strong Enhancer'"
     ]
    }
   ],
   "source": [
    "lm.fit(x_train, y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1afbd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lm.fit(x_train, y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1739156",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17760\\1413892101.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mMLA_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMLA_compare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MLA used'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLA_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mMLA_compare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Train Accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mMLA_compare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Test Accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m     \"\"\"\n\u001b[1;32m--> 962\u001b[1;33m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[0my_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} format is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: continuous format is not supported"
     ]
    }
   ],
   "source": [
    "    fp, tp, th = roc_curve(y_test, predicted)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'MLA used'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(x_train, y_train), 4)\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(x_test, y_test), 4)\n",
    "    MLA_compare.loc[row_index, 'Precission'] = precision_score(y_test, predicted)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = recall_score(y_test, predicted)\n",
    "    MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af50da8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Candidate Strong Enhancer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17760\\3560329855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mMLA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mMLA_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2075\u001b[0m             )\n\u001b[0;32m   2076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   2078\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Candidate Strong Enhancer'"
     ]
    }
   ],
   "source": [
    "row_index = 0\n",
    "for alg in MLA:  \n",
    "    \n",
    "    predicted = alg.fit(x_train, y_train).predict(x_test)\n",
    "    fp, tp, th = roc_curve(y_test, predicted)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'MLA used'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(x_train, y_train), 4)\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(x_test, y_test), 4)\n",
    "    MLA_compare.loc[row_index, 'Precission'] = precision_score(y_test, predicted)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = recall_score(y_test, predicted)\n",
    "    MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n",
    "\n",
    "    row_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c21d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ce65ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a6d5865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 LM predictions:  [0.34375, 0.28125, 0.24609375, 0.34375, 0.07421875]\n",
      "First 5 GBM predictions:  [0.3255569131908791, 0.17682304921118194, 0.2181580050764166, 0.31647069859187477, 0.06331650004906973]\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6debf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"dhs_0_1_wg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n",
    "\n",
    "# In[]:\n",
    "# Performing this same pipeline but using the extreme subsets of \"score\"\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetLower[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetLower[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetLower[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetLower[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetLower[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n",
    "\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetUpper[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetUpper[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetUpper[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetUpper[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetUpper[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n",
    "\n",
    "# In[]:\n",
    "    # After filtering for OCRs that are only present within certain TADs,\n",
    "    # reading in that dataset here and testing the harness again\n",
    "dataset=pd.read_csv('/Users/jamesonblount/Documents/Wray_Rotation/Carl/X_0012/data/OCRs_inTADs.csv')\n",
    "dataset.isnull().sum()\n",
    "\n",
    "datasetv2 = dataset.dropna(axis=1)\n",
    "datasetv2.isnull().sum()\n",
    "# Checking the data set for any NULL values is very essential, as MLAs can not \n",
    "# handle NULL values. We have to either eliminate the records with NULL values \n",
    "# or replace them with the mean/median of the other values. we can see each of \n",
    "# the variables are printed with number of null values. This data set has no null \n",
    "# values so all are zero here.\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
