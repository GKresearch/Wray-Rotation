{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b086488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 13 14:21:08 2022\n",
    "\n",
    "@author: jamesonblount\n",
    "\"\"\"\n",
    "\n",
    "# In[]:\n",
    "# Importing required packages\n",
    "#Importing basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing sklearn modules\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
    "from sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "717ec477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqnames                  0\n",
       "start                     0\n",
       "end                       0\n",
       "width                     0\n",
       "strand                    0\n",
       "                         ..\n",
       "gene.y                    0\n",
       "dTSS                      0\n",
       "PhastCons                 0\n",
       "PhyloP_primates_score     0\n",
       "PhyloP_placental_score    0\n",
       "Length: 90, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data and checking for missing values\n",
    "dataset=pd.read_csv('C:/Users/ictinike/Documents/WrayLab/raw_data/x_0011_df_phyloP.csv')\n",
    "dataset.isnull().sum()\n",
    "\n",
    "datasetv2 = dataset.dropna(axis=1)\n",
    "datasetv2.isnull().sum()\n",
    "# Checking the data set for any NULL values is very essential, as MLAs can not \n",
    "# handle NULL values. We have to either eliminate the records with NULL values \n",
    "# or replace them with the mean/median of the other values. we can see each of \n",
    "# the variables are printed with number of null values. This data set has no null \n",
    "# values so all are zero here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27b57d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Active Promoter', 'Candidate Strong Enhancer',\n",
       "        'Candidate Weak Enhancer', 'Distal CTCF/Candidate Insulator',\n",
       "        'Heterochromatin/Repetitive/Copy Number Variation',\n",
       "        'Inactive Promoter', 'Low activity proximal to active states',\n",
       "        'Polycomb repressed', 'Promoter Flanking',\n",
       "        'Transcription asociated'], dtype=object)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the categorical variables into numerical\n",
    "# Instantiate OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit_transform(datasetv2[[\"chromHMM_cat_longest\"]])[:5]\n",
    "datasetv2['chromHMM_cat_longest'].head()\n",
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7275d6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(datasetv2.chromHMM_cat_longest.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84160b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      seqnames      start        end  width strand       name  score  \\\n",
      "0         chr1     793301     793692    392      *     chr1.7    653   \n",
      "1         chr1     793779     794382    604      *     chr1.8    553   \n",
      "2         chr1     846424     847133    710      *    chr1.17    867   \n",
      "3         chr1     847379     847941    563      *    chr1.18    544   \n",
      "4         chr1     848207     849945   1739      *    chr1.19    605   \n",
      "...        ...        ...        ...    ...    ...        ...    ...   \n",
      "80348     chrX  155110643  155111395    753      *  chrX.2827   1000   \n",
      "80349     chrX  155196562  155196941    380      *  chrX.2829    677   \n",
      "80350     chrX  155227040  155227522    483      *  chrX.2831    619   \n",
      "80351     chrX  155231134  155231652    519      *  chrX.2832    745   \n",
      "80352     chrX  155232085  155232595    511      *  chrX.2833    688   \n",
      "\n",
      "       signalValue  pValue  qValue  ...  annotation_5' UTR  \\\n",
      "0           0.0677    3.87      -1  ...                  0   \n",
      "1           0.0359    1.98      -1  ...                  0   \n",
      "2           0.1357    7.97      -1  ...                  0   \n",
      "3           0.0332    1.82      -1  ...                  0   \n",
      "4           0.0525    2.96      -1  ...                  0   \n",
      "...            ...     ...     ...  ...                ...   \n",
      "80348       0.1998   11.90      -1  ...                  0   \n",
      "80349       0.0752    4.32      -1  ...                  0   \n",
      "80350       0.0570    3.23      -1  ...                  0   \n",
      "80351       0.0969    5.63      -1  ...                  1   \n",
      "80352       0.0789    4.54      -1  ...                  1   \n",
      "\n",
      "       annotation_Distal Intergenic  annotation_Downstream (1-2kb)  \\\n",
      "0                                 0                              0   \n",
      "1                                 0                              0   \n",
      "2                                 0                              0   \n",
      "3                                 0                              0   \n",
      "4                                 0                              0   \n",
      "...                             ...                            ...   \n",
      "80348                             0                              0   \n",
      "80349                             1                              0   \n",
      "80350                             0                              0   \n",
      "80351                             0                              0   \n",
      "80352                             0                              0   \n",
      "\n",
      "       annotation_Downstream (2-3kb)  annotation_Downstream (<1kb)  \\\n",
      "0                                  0                             0   \n",
      "1                                  0                             0   \n",
      "2                                  0                             0   \n",
      "3                                  0                             0   \n",
      "4                                  0                             0   \n",
      "...                              ...                           ...   \n",
      "80348                              0                             0   \n",
      "80349                              0                             0   \n",
      "80350                              0                             0   \n",
      "80351                              0                             0   \n",
      "80352                              0                             0   \n",
      "\n",
      "       annotation_Exon  annotation_Intron  annotation_Promoter (1-2kb)  \\\n",
      "0                    1                  0                            0   \n",
      "1                    1                  0                            0   \n",
      "2                    1                  0                            0   \n",
      "3                    1                  0                            0   \n",
      "4                    1                  0                            0   \n",
      "...                ...                ...                          ...   \n",
      "80348                0                  0                            0   \n",
      "80349                0                  0                            0   \n",
      "80350                0                  0                            0   \n",
      "80351                0                  0                            0   \n",
      "80352                0                  0                            0   \n",
      "\n",
      "       annotation_Promoter (2-3kb)  annotation_Promoter (<=1kb)  \n",
      "0                                0                            0  \n",
      "1                                0                            0  \n",
      "2                                0                            0  \n",
      "3                                0                            0  \n",
      "4                                0                            0  \n",
      "...                            ...                          ...  \n",
      "80348                            0                            1  \n",
      "80349                            0                            0  \n",
      "80350                            0                            1  \n",
      "80351                            0                            0  \n",
      "80352                            0                            0  \n",
      "\n",
      "[80353 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoded_data = pd.get_dummies(datasetv2, columns = ['chromHMM_cat_longest','annotation'])\n",
    "print(one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbc01a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ictinike\\AppData\\Local\\Temp\\ipykernel_8624\\3935728989.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  datasetv2['score_quart'] = pd.qcut(datasetv2['score'], q = 4, labels = bin_labels)\n"
     ]
    }
   ],
   "source": [
    "# Compare performance of ML between top quartile to bottom quartile of AUC (best)/peak size\n",
    "bin_labels = ['Lower', 'Midlower','Midupper', 'Upper']\n",
    "lower_bin = ['Lower']\n",
    "upper_bin = ['Upper']\n",
    "datasetv2['score_quart'] = pd.qcut(datasetv2['score'], q = 4, labels = bin_labels)\n",
    "datasetLower = datasetv2[datasetv2['score_quart'].isin(lower_bin)]\n",
    "datasetUpper = datasetv2[datasetv2['score_quart'].isin(upper_bin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e78e3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetv2[\"dhs_0_1_wg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05dd7fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(sparse=False),\n",
       "                                 ['chromHMM_cat_longest', 'annotation'])])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db4006bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "faecdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7ce65ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a6d5865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 LM predictions:  [0.34375, 0.28125, 0.24609375, 0.34375, 0.07421875]\n",
      "First 5 GBM predictions:  [0.3255569131908792, 0.17682304921118194, 0.21815800507641653, 0.31647069859187493, 0.0633165000490697]\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d6debf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM MAE: 1.28\n",
      "LM RMSE: 2.00\n",
      "GBM MAE: 1.29\n",
      "GBM RMSE: 1.99\n"
     ]
    }
   ],
   "source": [
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "10b56d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Candidate Strong Enhancer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8624\\1886847114.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Candidate Strong Enhancer'"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0fd9168c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Candidate Strong Enhancer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8624\\623679559.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Candidate Strong Enhancer'"
     ]
    }
   ],
   "source": [
    "rf = make_pipeline(column_transform, RandomForestRegressor)\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################### Permutation feature importance #####################################\n",
    "\n",
    "imp = rfpimp.importances(rf, X_test, y_test)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "ax.barh(imp.index, imp['Importance'], height=0.8, facecolor='grey', alpha=0.8, edgecolor='k')\n",
    "ax.set_xlabel('Importance score')\n",
    "ax.set_title('Permutation feature importance')\n",
    "ax.text(0.8, 0.15, 'aegis4048.github.io', fontsize=12, ha='center', va='center',\n",
    "        transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c63d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0cf89755",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8624\\3283068318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plt.scatter(gbm_predictions, y_test, color = \"red\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"green\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#plt.title(\"Salary vs Experience (Training set)\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#plt.xlabel(\"Years of Experience\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#plt.ylabel(\"Salary\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mVector\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mthis\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1222\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "\n",
    "#plt.scatter(gbm_predictions, y_test, color = \"red\")\n",
    "plt.plot(x_train, lr.predict(x_train), color = \"green\")\n",
    "#plt.title(\"Salary vs Experience (Training set)\")\n",
    "#plt.xlabel(\"Years of Experience\")\n",
    "#plt.ylabel(\"Salary\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc0fd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"dhs_0_1_wg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a4a9bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(sparse=False),\n",
       "                                 ['chromHMM_cat_longest', 'annotation'])])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "245e409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 LR predictions:  [0, 0, 0, 0, 0]\n",
      "First 5 SVM predictions:  [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7dffe728",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_pipeline = make_pipeline(column_transform, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0f1c7955",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8624\\209941211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrf_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrf_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"First 5 RF predictions: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# To use random forest, need binary outcome\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;34m\"multilabel-sequences\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     ]:\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "rf_pipeline.fit(x_train, y_train)\n",
    "rf_predictions = rf_pipeline.predict(x_test)\n",
    "print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04451f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing this same pipeline but using the extreme subsets of \"score\"\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetLower[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetLower[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetLower[\"dhs_0_1_wg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04544a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetLower[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetLower[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n",
    "\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetUpper[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetUpper[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetUpper[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetUpper[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetUpper[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n",
    "\n",
    "# In[]:\n",
    "    # After filtering for OCRs that are only present within certain TADs,\n",
    "    # reading in that dataset here and testing the harness again\n",
    "dataset=pd.read_csv('C:/Users/Ictinike/Documents/WrayLab/raw_data/OCRs_inTADs.csv')\n",
    "dataset.isnull().sum()\n",
    "\n",
    "datasetv2 = dataset.dropna(axis=1)\n",
    "datasetv2.isnull().sum()\n",
    "# Checking the data set for any NULL values is very essential, as MLAs can not \n",
    "# handle NULL values. We have to either eliminate the records with NULL values \n",
    "# or replace them with the mean/median of the other values. we can see each of \n",
    "# the variables are printed with number of null values. This data set has no null \n",
    "# values so all are zero here.\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
