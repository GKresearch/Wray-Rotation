{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5e1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 13 14:21:08 2022\n",
    "\n",
    "@author: jamesonblount\n",
    "\"\"\"\n",
    "\n",
    "# In[]:\n",
    "# Importing required packages\n",
    "#Importing basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing sklearn modules\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
    "from sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc9f8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqnames                  0\n",
       "start                     0\n",
       "end                       0\n",
       "width                     0\n",
       "strand                    0\n",
       "                         ..\n",
       "gene.y                    0\n",
       "dTSS                      0\n",
       "PhastCons                 0\n",
       "PhyloP_primates_score     0\n",
       "PhyloP_placental_score    0\n",
       "Length: 90, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[]:\n",
    "#Loading the data and checking for missing values\n",
    "dataset=pd.read_csv('C:/Users/ictinike/Documents/WrayLab/raw_data/x_0011_df_phyloP.csv')\n",
    "dataset.isnull().sum()\n",
    "\n",
    "datasetv2 = dataset.dropna(axis=1)\n",
    "datasetv2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083cff74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqnames</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>width</th>\n",
       "      <th>strand</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>signalValue</th>\n",
       "      <th>pValue</th>\n",
       "      <th>qValue</th>\n",
       "      <th>...</th>\n",
       "      <th>S_rare</th>\n",
       "      <th>eQTLs</th>\n",
       "      <th>PP_con</th>\n",
       "      <th>PP_acc</th>\n",
       "      <th>gene.y</th>\n",
       "      <th>dTSS</th>\n",
       "      <th>PhastCons</th>\n",
       "      <th>PhyloP_mammals_score</th>\n",
       "      <th>PhyloP_primates_score</th>\n",
       "      <th>PhyloP_placental_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>793301</td>\n",
       "      <td>793692</td>\n",
       "      <td>392</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.7</td>\n",
       "      <td>653</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>3.87</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951590</td>\n",
       "      <td>0.293804</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-67621</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.164622</td>\n",
       "      <td>-0.078358</td>\n",
       "      <td>-0.103762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>793779</td>\n",
       "      <td>794382</td>\n",
       "      <td>604</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.8</td>\n",
       "      <td>553</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>1.98</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.948498</td>\n",
       "      <td>0.189047</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-67037</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.060388</td>\n",
       "      <td>-0.150270</td>\n",
       "      <td>-0.147019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>846424</td>\n",
       "      <td>847133</td>\n",
       "      <td>710</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.17</td>\n",
       "      <td>867</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>7.97</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.927576</td>\n",
       "      <td>0.266338</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-14339</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.080633</td>\n",
       "      <td>-0.131020</td>\n",
       "      <td>-0.100292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>847379</td>\n",
       "      <td>847941</td>\n",
       "      <td>563</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.18</td>\n",
       "      <td>544</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.143830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-13458</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.172059</td>\n",
       "      <td>-0.024884</td>\n",
       "      <td>-0.058467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>848207</td>\n",
       "      <td>849945</td>\n",
       "      <td>1739</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.19</td>\n",
       "      <td>605</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>2.96</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>0.342990</td>\n",
       "      <td>0.833725</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-12042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.032988</td>\n",
       "      <td>-0.135259</td>\n",
       "      <td>-0.177847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  seqnames   start     end  width strand     name  score  signalValue  pValue  \\\n",
       "0     chr1  793301  793692    392      *   chr1.7    653       0.0677    3.87   \n",
       "1     chr1  793779  794382    604      *   chr1.8    553       0.0359    1.98   \n",
       "2     chr1  846424  847133    710      *  chr1.17    867       0.1357    7.97   \n",
       "3     chr1  847379  847941    563      *  chr1.18    544       0.0332    1.82   \n",
       "4     chr1  848207  849945   1739      *  chr1.19    605       0.0525    2.96   \n",
       "\n",
       "   qValue  ...  S_rare  eQTLs    PP_con    PP_acc  gene.y   dTSS  PhastCons  \\\n",
       "0      -1  ...       5      0  0.951590  0.293804  SAMD11 -67621      0.003   \n",
       "1      -1  ...      11      4  0.948498  0.189047  SAMD11 -67037      0.001   \n",
       "2      -1  ...      10      5  0.927576  0.266338  SAMD11 -14339      0.000   \n",
       "3      -1  ...       1      2  0.143830  1.000000  SAMD11 -13458      0.001   \n",
       "4      -1  ...      24     11  0.342990  0.833725  SAMD11 -12042      0.000   \n",
       "\n",
       "   PhyloP_mammals_score  PhyloP_primates_score  PhyloP_placental_score  \n",
       "0             -0.164622              -0.078358               -0.103762  \n",
       "1             -0.060388              -0.150270               -0.147019  \n",
       "2             -0.080633              -0.131020               -0.100292  \n",
       "3             -0.172059              -0.024884               -0.058467  \n",
       "4             -0.032988              -0.135259               -0.177847  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca580f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Active Promoter', 'Candidate Strong Enhancer',\n",
       "        'Candidate Weak Enhancer', 'Distal CTCF/Candidate Insulator',\n",
       "        'Heterochromatin/Repetitive/Copy Number Variation',\n",
       "        'Inactive Promoter', 'Low activity proximal to active states',\n",
       "        'Polycomb repressed', 'Promoter Flanking',\n",
       "        'Transcription asociated'], dtype=object)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Transforming the categorical variables into numerical\n",
    "# Instantiate OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit_transform(datasetv2[[\"chromHMM_cat_longest\"]])[:5]\n",
    "datasetv2['chromHMM_cat_longest'].head()\n",
    "ohe.categories_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c53aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromHMM_cat_longest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distal CTCF/Candidate Insulator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate Weak Enhancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate Weak Enhancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate Weak Enhancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polycomb repressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80348</th>\n",
       "      <td>Active Promoter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80349</th>\n",
       "      <td>Distal CTCF/Candidate Insulator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80350</th>\n",
       "      <td>Candidate Strong Enhancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80351</th>\n",
       "      <td>Candidate Strong Enhancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80352</th>\n",
       "      <td>Candidate Strong Enhancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80353 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  chromHMM_cat_longest\n",
       "0      Distal CTCF/Candidate Insulator\n",
       "1              Candidate Weak Enhancer\n",
       "2              Candidate Weak Enhancer\n",
       "3              Candidate Weak Enhancer\n",
       "4                   Polycomb repressed\n",
       "...                                ...\n",
       "80348                  Active Promoter\n",
       "80349  Distal CTCF/Candidate Insulator\n",
       "80350        Candidate Strong Enhancer\n",
       "80351        Candidate Strong Enhancer\n",
       "80352        Candidate Strong Enhancer\n",
       "\n",
       "[80353 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetv2[[\"chromHMM_cat_longest\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bccecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(datasetv2[[\"chromHMM_cat_longest\"]])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3047a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ictinike\\AppData\\Local\\Temp\\ipykernel_14592\\2677276402.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  datasetv2['score_quart'] = pd.qcut(datasetv2['score'], q = 4, labels = bin_labels)\n"
     ]
    }
   ],
   "source": [
    "# In[]:\n",
    "# Compare performance of ML between top quartile to bottom quartile of AUC (best)/peak size\n",
    "bin_labels = ['Lower', 'Midlower','Midupper', 'Upper']\n",
    "lower_bin = ['Lower']\n",
    "upper_bin = ['Upper']\n",
    "datasetv2['score_quart'] = pd.qcut(datasetv2['score'], q = 4, labels = bin_labels)\n",
    "datasetLower = datasetv2[datasetv2['score_quart'].isin(lower_bin)]\n",
    "datasetUpper = datasetv2[datasetv2['score_quart'].isin(upper_bin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecb2729d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqnames</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>width</th>\n",
       "      <th>strand</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>signalValue</th>\n",
       "      <th>pValue</th>\n",
       "      <th>qValue</th>\n",
       "      <th>...</th>\n",
       "      <th>S_rare</th>\n",
       "      <th>eQTLs</th>\n",
       "      <th>PP_con</th>\n",
       "      <th>PP_acc</th>\n",
       "      <th>gene.y</th>\n",
       "      <th>dTSS</th>\n",
       "      <th>PhastCons</th>\n",
       "      <th>PhyloP_primates_score</th>\n",
       "      <th>PhyloP_placental_score</th>\n",
       "      <th>score_quart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>846424</td>\n",
       "      <td>847133</td>\n",
       "      <td>710</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.17</td>\n",
       "      <td>867</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>7.97</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.927576</td>\n",
       "      <td>0.266338</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-14339</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131020</td>\n",
       "      <td>-0.100292</td>\n",
       "      <td>Upper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr1</td>\n",
       "      <td>856118</td>\n",
       "      <td>856944</td>\n",
       "      <td>827</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.24</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3804</td>\n",
       "      <td>16.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.376009</td>\n",
       "      <td>0.912880</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-4587</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.056528</td>\n",
       "      <td>-0.023788</td>\n",
       "      <td>Upper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chr1</td>\n",
       "      <td>858457</td>\n",
       "      <td>863327</td>\n",
       "      <td>4871</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.27</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>12.70</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>0.668905</td>\n",
       "      <td>0.620494</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>-226</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.072562</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>Upper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chr1</td>\n",
       "      <td>870706</td>\n",
       "      <td>881162</td>\n",
       "      <td>10457</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.34</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.4497</td>\n",
       "      <td>16.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>38</td>\n",
       "      <td>0.238055</td>\n",
       "      <td>0.840641</td>\n",
       "      <td>SAMD11</td>\n",
       "      <td>14816</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.079952</td>\n",
       "      <td>-0.064100</td>\n",
       "      <td>Upper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chr1</td>\n",
       "      <td>893900</td>\n",
       "      <td>897154</td>\n",
       "      <td>3255</td>\n",
       "      <td>*</td>\n",
       "      <td>chr1.40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>16.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>0.877861</td>\n",
       "      <td>0.309112</td>\n",
       "      <td>KLHL17</td>\n",
       "      <td>-440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.151414</td>\n",
       "      <td>-0.121736</td>\n",
       "      <td>Upper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seqnames   start     end  width strand     name  score  signalValue  \\\n",
       "2      chr1  846424  847133    710      *  chr1.17    867       0.1357   \n",
       "9      chr1  856118  856944    827      *  chr1.24   1000       0.3804   \n",
       "12     chr1  858457  863327   4871      *  chr1.27   1000       0.2138   \n",
       "14     chr1  870706  881162  10457      *  chr1.34   1000       0.4497   \n",
       "18     chr1  893900  897154   3255      *  chr1.40   1000       0.4485   \n",
       "\n",
       "    pValue  qValue  ...  S_rare  eQTLs    PP_con    PP_acc  gene.y   dTSS  \\\n",
       "2     7.97      -1  ...      10      5  0.927576  0.266338  SAMD11 -14339   \n",
       "9    16.00      -1  ...      10      5  0.376009  0.912880  SAMD11  -4587   \n",
       "12   12.70      -1  ...      50     14  0.668905  0.620494  SAMD11   -226   \n",
       "14   16.00      -1  ...     177     38  0.238055  0.840641  SAMD11  14816   \n",
       "18   16.00      -1  ...      39     14  0.877861  0.309112  KLHL17   -440   \n",
       "\n",
       "    PhastCons  PhyloP_primates_score  PhyloP_placental_score  score_quart  \n",
       "2       0.000              -0.131020               -0.100292        Upper  \n",
       "9       0.001              -0.056528               -0.023788        Upper  \n",
       "12      0.001              -0.072562                0.036639        Upper  \n",
       "14      0.000              -0.079952               -0.064100        Upper  \n",
       "18      0.000              -0.151414               -0.121736        Upper  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetUpper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0aaf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"wgCERES_score_nosig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f06eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af496657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93bd2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d1f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 LM predictions:  [0.34375, 0.28125, 0.24609375, 0.34375, 0.07421875]\n",
      "First 5 GBM predictions:  [0.3255569131908791, 0.17682304921118197, 0.21815800507641656, 0.3164706985918748, 0.06331650004906972]\n"
     ]
    }
   ],
   "source": [
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "042e30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM MAE: 1.28\n",
      "LM RMSE: 2.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f1964d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM MAE: 1.29\n",
      "GBM RMSE: 1.99\n"
     ]
    }
   ],
   "source": [
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ccbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing this same pipeline but using the extreme subsets of \"score\"\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetLower[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetLower[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetLower[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fca206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetLower[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetLower[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b36704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetUpper[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetUpper[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetUpper[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99210f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c20d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ce172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06becee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetUpper[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetUpper[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a989a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f2151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e63396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208102fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "    # After filtering for OCRs that are only present within certain TADs,\n",
    "    # reading in that dataset here and testing the harness again\n",
    "dataset=pd.read_csv('/Users/jamesonblount/Documents/Wray_Rotation/Carl/X_0012/data/OCRs_inTADs.csv')\n",
    "dataset.isnull().sum()\n",
    "\n",
    "datasetv2 = dataset.dropna(axis=1)\n",
    "datasetv2.isnull().sum()\n",
    "# Checking the data set for any NULL values is very essential, as MLAs can not \n",
    "# handle NULL values. We have to either eliminate the records with NULL values \n",
    "# or replace them with the mean/median of the other values. we can see each of \n",
    "# the variables are printed with number of null values. This data set has no null \n",
    "# values so all are zero here.\n",
    "# In[]:\n",
    "    # here we'll start by using wgCERES_score_nosig as the response vector,\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"wgCERES_score_nosig\"]\n",
    "# For classifier ML techniques, i.e. SVM and Random Forest\n",
    "# y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "\n",
    "# In[]:\n",
    "# Building the rest of the pipeline\n",
    "# Instantiate pipeline with linear regression\n",
    "lm  = LinearRegression()\n",
    "lm_pipeline = make_pipeline(column_transform, lm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with gradient boosting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(column_transform, gbm)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline with logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_pipeline = make_pipeline(column_transform, lr)\n",
    "\n",
    "# In[]:\n",
    "# Instantiate pipeline for SVM\n",
    "sv = SVC()\n",
    "sv_pipeline = make_pipeline(column_transform, sv)\n",
    "\n",
    "# In[]:\n",
    "# Fit pipeline to training set and make predictions on test set\n",
    "\n",
    "lm_pipeline.fit(x_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(x_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(x_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(x_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lm_mae = mean_absolute_error(lm_predictions, y_test)\n",
    "lm_rmse = np.sqrt(mean_squared_error(lm_predictions, y_test))\n",
    "print(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\n",
    "print(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n",
    "\n",
    "gbm_mae = mean_absolute_error(gbm_predictions, y_test)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(gbm_predictions, y_test))\n",
    "print(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\n",
    "print(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))\n",
    "\n",
    "\n",
    "# In[]:\n",
    "    # here we'll use dhs_0_1_wg as the response vector for the classifiers\n",
    "x = datasetv2[[\"DHS_prop_repeat\", \n",
    "                    \"DHS_prop_GC\", \"DHS_length\", \"n_SNV_Zhou_per_bp\", \n",
    "                    \"distanceToTSS\", \"zeta.human\", \"zeta.chimp\", \"PP_con\", \"PP_acc\", \n",
    "                    \"PhastCons\",\n",
    "                    \"chromHMM_cat_longest\", \n",
    "                    \"annotation\", \"PhyloP_primates_score\"]]\n",
    "y = datasetv2[\"dhs_0_1_wg\"]\n",
    "\n",
    "# In[]:\n",
    "    # Make column transformer\n",
    "column_transform = []\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['chromHMM_cat_longest','annotation']))\n",
    "\n",
    "#Apply column transformer to predictor variables\n",
    "column_transform.fit(x)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "# Splitting train and split data\n",
    "# The test data set size is 20% of the total records. This test data will not \n",
    "# be used in model training and work as an independent test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "# In[]:\n",
    "\n",
    "lr_pipeline.fit(x_train, y_train)\n",
    "lr_predictions = lr_pipeline.predict(x_test)\n",
    "print(\"First 5 LR predictions: \", list(lr_predictions[:5]))\n",
    "\n",
    "sv_pipeline.fit(x_train, y_train)\n",
    "sv_predictions = sv_pipeline.predict(x_test)\n",
    "print(\"First 5 SVM predictions: \", list(sv_predictions[:5]))\n",
    "\n",
    "#rf_pipeline.fit(x_train, y_train)\n",
    "#rf_predictions = rf_pipeline.predict(x_test)\n",
    "#print(\"First 5 RF predictions: \", list(rf_predictions[:5]))\n",
    "# To use random forest, need binary outcome\n",
    "\n",
    "# In[]:\n",
    "# With predictions ready from the two pipelines, we can proceed to evaluate the \n",
    "# accuracy of these predictions using mean absolute error (MAE) and mean squared \n",
    "# error (RMSE).\n",
    "# Calculate mean square error and root mean squared error\n",
    "\n",
    "lr_mae = mean_absolute_error(lr_predictions, y_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(lr_predictions, y_test))\n",
    "print(\"LR MAE: {:.2f}\".format(round(lr_mae, 2)))\n",
    "print(\"LR RMSE: {:.2f}\".format(round(lr_rmse, 2)))\n",
    "\n",
    "sv_mae = mean_absolute_error(sv_predictions, y_test)\n",
    "sv_rmse = np.sqrt(mean_squared_error(sv_predictions, y_test))\n",
    "print(\"SVM MAE: {:.2f}\".format(round(sv_mae, 2)))\n",
    "print(\"SVM RMSE: {:.2f}\".format(round(sv_rmse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d133e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
